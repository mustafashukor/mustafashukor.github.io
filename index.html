<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mustafa Shukor</title>
    <!-- Import IBM Plex Mono font from Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
      
<body>
  <div class="container">
    <h1>Mustafa Shukor</h1>
    <p class="description">CS PhD at Sorbonne.</p> <!-- Add your description here -->
    <div class="links">
      <a href="https://scholar.google.com/citations?user=lhp9mRgAAAAJ&hl=en" target="_blank" aria-label="Google Scholar">
          <img src="assets/googlescholar.svg" alt="Google Scholar logo">
      </a>
      <a href="https://github.com/mshukor" target="_blank" aria-label="Github">
          <img src="assets/github.svg" alt="Github">
      </a>
      <a href="https://huggingface.co/mshukor" target="_blank" aria-label="HF">
          <img src="assets/hf-logo-pirate.svg" class="hf" alt="HF">
      </a>
      <a href="https://x.com/MustafaShukor1" target="_blank" aria-label="Twitter">
          <img src="assets/x.svg" alt="Twitter logo">
      </a>
      <a href="mailto:mshukor.contact@gmail.com" aria-label="Email">
          <img src="assets/gmail.svg" alt="Email logo">
      </a>
    </div>

    <br><br><br>
    <div class="news">
      <h2>Updates</h2><br>

  
      <ul>
        <li><span class="date">2025-07:</span> We released our work on <a href="https://arxiv.org/abs/2507.09404" target="_blank" class="highlight-link">Scaling laws for optimal data mixtures</a>.
      </ul>

      <ul>
        <li><span class="date">2025-07:</span> Our work on <a href="https://arxiv.org/abs/2504.07951" target="_blank" class="highlight-link">Scaling laws for native multimodal models</a> got an Oral (~top 2%) at ICCV 2025.
      </ul>

      <ul>
        <li><span class="date">2025-07:</span> 2 papers accepted at ICCV 2025: <a href="https://arxiv.org/abs/2504.07951" target="_blank" class="highlight-link">Scaling laws for NMMs</a> and <a href="https://arxiv.org/abs/2504.07951" target="_blank" class="highlight-link">Multimodal Steering</a>.
      </ul>

      <ul>
        <li><span class="date">2025-06:</span> We released <a href="https://x.com/MustafaShukor1/status/1929796105782329818" target="_blank" class="highlight-link">SmolVLA</a> an efficient foundation model for robotics:  <a href="https://arxiv.org/abs/2506.01844" target="_blank" class="highlight-link">paper</a>,  <a href="https://github.com/huggingface/lerobot/pull/1175" target="_blank" class="highlight-link">code</a>, and <a href="https://huggingface.co/blog/smolvla" target="_blank" class="highlight-link">blogpost</a>.
      </ul>

      <ul>
        <li><span class="date">2025-02:</span> <a href="https://arxiv.org/abs/2411.14402" target="_blank" class="highlight-link">AIMv2</a> is accepted as a Spotlight paper (~top 5%) at CVPR 2025. <br>
      </ul>

      <ul>
        <li><span class="date">2024-11:</span> We released <a href="https://github.com/apple/ml-aim" target="_blank" class="highlight-link">AIMv2</a>, a SoTA large-scale vision encoder. <br>
      </ul>

      <ul>
        <li><span class="date">2024-09:</span> 3 papers accepted at NeurIPS 2024 + and 1 at the RBFM Workshop:
          <a href="https://example.com/ima-paper" target="_blank" class="highlight-link">Implicit Multimodal Alignment</a>, 
          <a href="https://example.com/ima-paper" target="_blank" class="highlight-link">CoX-LMM</a>, 
          <a href="https://example.com/ima-paper" target="_blank" class="highlight-link">DiffCut</a> and 
          <a href="https://example.com/ima-paper" target="_blank" class="highlight-link">Skipping Computations</a>.</li>
      </ul>

      <ul>
        <li><span class="date">2024-04:</span> <a href="https://arxiv.org/abs/2310.00647" target="_blank" class="highlight-link">Beyond task performance</a> is accepted at ICLR 2024.
      </ul>
      
      <ul>
        <li><span class="date">2023-12:</span> <a href="https://arxiv.org/abs/2307.16184" target="_blank" class="highlight-link">UnIVAL</a>: Unified Model for Image, Video, Audio and Language Tasks, is accepted at TMLR 2023: <a href="https://arxiv.org/abs/2307.16184" target="_blank" class="highlight-link">paper</a> and <a href="https://github.com/mshukor/UnIVAL" target="_blank" class="highlight-link">code</a>.
      </ul>

      <ul>
        <li><span class="date">2023-09:</span> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html" target="_blank" class="highlight-link">Rewarded soups</a> is accepted at NeurIPS 2023.
      </ul>
      

    </div>

  </div>
</body>
</html>

